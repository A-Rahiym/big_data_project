{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sofia Air Quality ETL Notebook\n",
    "\n",
    "This notebook implements the end-to-end data pipeline for the Sofia air quality dataset and is Colab-friendly.\n",
    "\n",
    "Run cells from top to bottom in order. The steps are:\n",
    "\n",
    "1. Install runtime dependencies (Colab).\n",
    "2. (Optional) Example Spark ETL script that demonstrates processing inside a Kaggle/Colab environment.\n",
    "3. Install additional packages used by the helper cells.\n",
    "4. Upload `kaggle.json` and download the dataset into `data/raw/`.\n",
    "5. Run the repository ETL (`processing/etl_pipeline.py`) which writes partitioned Parquet into `data/processed/sofia_air_quality_weather`.\n",
    "6. Upload the processed output to S3.\n",
    "\n",
    "Notes:\n",
    "- In Colab you will be asked to upload `kaggle.json` (from Kaggle -> Account -> Create New API Token).\n",
    "- If you prefer, run the same ETL locally by running `python run_pipeline.py` or calling `processing/etl_pipeline.run_etl()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-03T07:59:09.332685Z",
     "iopub.status.busy": "2025-09-03T07:59:09.332373Z",
     "iopub.status.idle": "2025-09-03T07:59:17.393353Z",
     "shell.execute_reply": "2025-09-03T07:59:17.392176Z",
     "shell.execute_reply.started": "2025-09-03T07:59:09.332654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Example: Spark ETL (Kaggle/Colab example)\n",
    "\n",
    "The cell below demonstrates how you might run a PySpark-based ETL inside a notebook environment where the dataset is already available (for example, in a Kaggle kernel). If you are running the repo ETL (recommended) skip this example and run the later `processing/etl_pipeline.run_etl()` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark plotly streamlit\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, avg, year, month\n",
    "import os\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"Sofia Air Quality + Weather ETL\").getOrCreate()\n",
    "\n",
    "# Base dataset path\n",
    "base_path = \"/kaggle/input/sofia-air-quality-dataset/\"\n",
    "\n",
    "# --- WEATHER FILES ---\n",
    "weather_files = [\n",
    "    os.path.join(base_path, f) for f in os.listdir(base_path)\n",
    "    if \"bme280sof\" in f and os.path.getsize(os.path.join(base_path, f)) > 0\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(weather_files)} weather files with data\")\n",
    "\n",
    "df_weather = spark.read.csv(weather_files, header=True, inferSchema=True)\n",
    "df_weather = df_weather.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "df_weather = df_weather.select(\n",
    "    \"timestamp\", \"location\", \"lat\", \"lon\", \"date\", \"temperature\", \"humidity\", \"pressure\"\n",
    ").dropna()\n",
    "\n",
    "# --- POLLUTANT FILES ---\n",
    "pollutant_files = [\n",
    "    os.path.join(base_path, f) for f in os.listdir(base_path)\n",
    "    if \"sds011sof\" in f and os.path.getsize(os.path.join(base_path, f)) > 0\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(pollutant_files)} pollutant files with data\")\n",
    "\n",
    "df_pollution = spark.read.csv(pollutant_files, header=True, inferSchema=True)\n",
    "df_pollution = df_pollution.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "df_pollution = df_pollution.select(\n",
    "    \"timestamp\", \"location\", \"lat\", \"lon\", \"date\",\n",
    "    col(\"P1\").alias(\"PM10\"), col(\"P2\").alias(\"PM2_5\")\n",
    ").dropna()\n",
    "\n",
    "# --- JOIN POLLUTION + WEATHER ---\n",
    "df_joined = df_pollution.join(\n",
    "    df_weather,\n",
    "    [\"timestamp\", \"location\", \"lat\", \"lon\", \"date\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# --- DAILY AVERAGES ---\n",
    "daily_avg = df_joined.groupBy(\"location\", \"lat\", \"lon\", \"date\").agg(\n",
    "    avg(\"PM10\").alias(\"avg_PM10\"),\n",
    "    avg(\"PM2_5\").alias(\"avg_PM2_5\"),\n",
    "    avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "    avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "    avg(\"pressure\").alias(\"avg_pressure\")\n",
    ").withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "# Save parquet\n",
    "output_path = \"/kaggle/working/sofia_air_quality_weather.parquet_updated\"\n",
    "daily_avg.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(output_path)\n",
    "\n",
    "spark.stop()\n",
    "print(f\"✅ Processed data saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T07:59:22.972758Z",
     "iopub.status.busy": "2025-09-03T07:59:22.971828Z",
     "iopub.status.idle": "2025-09-03T08:13:49.881085Z",
     "shell.execute_reply": "2025-09-03T08:13:49.879370Z",
     "shell.execute_reply.started": "2025-09-03T07:59:22.972712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, avg, year, month\n",
    "import os\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"Sofia Air Quality + Weather ETL\").getOrCreate()\n",
    "\n",
    "# Base dataset path\n",
    "base_path = \"/kaggle/input/sofia-air-quality-dataset/\"\n",
    "\n",
    "# --- WEATHER FILES ---\n",
    "weather_files = [\n",
    "    os.path.join(base_path, f) for f in os.listdir(base_path)\n",
    "    if \"bme280sof\" in f and os.path.getsize(os.path.join(base_path, f)) > 0\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(weather_files)} weather files with data\")\n",
    "\n",
    "df_weather = spark.read.csv(weather_files, header=True, inferSchema=True)\n",
    "df_weather = df_weather.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "df_weather = df_weather.select(\n",
    "    \"timestamp\", \"location\", \"lat\", \"lon\", \"date\", \"temperature\", \"humidity\", \"pressure\"\n",
    ").dropna()\n",
    "\n",
    "# --- POLLUTANT FILES ---\n",
    "pollutant_files = [\n",
    "    os.path.join(base_path, f) for f in os.listdir(base_path)\n",
    "    if \"sds011sof\" in f and os.path.getsize(os.path.join(base_path, f)) > 0\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(pollutant_files)} pollutant files with data\")\n",
    "\n",
    "df_pollution = spark.read.csv(pollutant_files, header=True, inferSchema=True)\n",
    "df_pollution = df_pollution.withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "df_pollution = df_pollution.select(\n",
    "    \"timestamp\", \"location\", \"lat\", \"lon\", \"date\",\n",
    "    col(\"P1\").alias(\"PM10\"), col(\"P2\").alias(\"PM2_5\")\n",
    ").dropna()\n",
    "\n",
    "# --- JOIN POLLUTION + WEATHER ---\n",
    "df_joined = df_pollution.join(\n",
    "    df_weather,\n",
    "    [\"timestamp\", \"location\", \"lat\", \"lon\", \"date\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# --- DAILY AVERAGES ---\n",
    "daily_avg = df_joined.groupBy(\"location\", \"lat\", \"lon\", \"date\").agg(\n",
    "    avg(\"PM10\").alias(\"avg_PM10\"),\n",
    "    avg(\"PM2_5\").alias(\"avg_PM2_5\"),\n",
    "    avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "    avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "    avg(\"pressure\").alias(\"avg_pressure\")\n",
    ").withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "# Save parquet\n",
    "output_path = \"/kaggle/working/sofia_air_quality_weather.parquet_updated\"\n",
    "daily_avg.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(output_path)\n",
    "\n",
    "spark.stop()\n",
    "print(f\"✅ Processed data saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install dependencies (Colab)\n",
    "\n",
    "This cell installs the Python packages required when running in Google Colab: PySpark (for ETL), Kaggle client, boto3 for S3 uploads, and plotting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime dependencies (Colab)\n",
    "!pip install -q pyspark plotly streamlit boto3 kaggle pyarrow s3fs\n",
    "print(\"✅ Dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload kaggle.json and download Kaggle dataset (Colab)\n",
    "from google.colab import files\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Upload your kaggle.json (from Kaggle -> Account -> Create New API Token)\")\n",
    "uploaded = files.upload()\n",
    "if 'kaggle.json' in uploaded:\n",
    "    with open('kaggle.json', 'wb') as f:\n",
    "        f.write(uploaded['kaggle.json'])\n",
    "    os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "    print('Saved kaggle.json to current working directory')\n",
    "else:\n",
    "    print('kaggle.json not uploaded, please upload and rerun this cell')\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# download dataset into data/raw\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "api.dataset_download_files('hmavrodiev/sofia-air-quality-dataset', path='data/raw', unzip=True)\n",
    "print('✅ Dataset downloaded to data/raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL pipeline (uses processing/etl_pipeline.py)\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from processing.etl_pipeline import run_etl\n",
    "\n",
    "# ensure raw data exists\n",
    "if not os.path.exists('data/raw'):\n",
    "    raise RuntimeError('data/raw is missing — run the previous cell to download the dataset')\n",
    "\n",
    "run_etl(input_path='data/raw', output_path='data/processed/sofia_air_quality_weather')\n",
    "print('✅ ETL completed and processed data saved to data/processed/sofia_air_quality_weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload processed data to S3\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket_name = 'my-sofia-air-quality'  # CHANGE to your bucket\n",
    "prefix = 'processed-data/'\n",
    "\n",
    "# walk data/processed and upload files\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk('data/processed/sofia_air_quality_weather'):\n",
    "    for fname in files:\n",
    "        local_path = os.path.join(root, fname)\n",
    "        rel_path = os.path.relpath(local_path, 'data/processed/sofia_air_quality_weather')\n",
    "        s3_key = os.path.join(prefix, rel_path).replace('\\\\', '/')\n",
    "        print(f'Uploading {local_path} -> s3://{bucket_name}/{s3_key}')\n",
    "        s3.upload_file(local_path, bucket_name, s3_key)\n",
    "\n",
    "print('✅ Upload completed')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 340483,
     "sourceId": 684093,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
